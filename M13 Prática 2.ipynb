{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OS02nR3zTw0f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar os dados\n",
        "df = pd.read_csv(\"/content/previsao_de_renda.csv\")"
      ],
      "metadata": {
        "id": "xRgkQJufUYWI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar os dados\n",
        "X = df.drop('renda', axis=1)\n",
        "y = df['renda']"
      ],
      "metadata": {
        "id": "c1nslPc1Ud0O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter variáveis categóricas em dummies\n",
        "X = pd.get_dummies(X, drop_first=True)"
      ],
      "metadata": {
        "id": "WJVMBfGnUft1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "26QsCeyKUhdl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "source": [
        "# Escalar os dados\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Ajustar e transformar o escalonador nos dados de treinamento\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transforme os dados de teste usando o mesmo scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Verifique os valores NaN nos dados dimensionados\n",
        "print(np.isnan(X_train_scaled).any())\n",
        "print(np.isnan(X_test_scaled).any())\n",
        "\n",
        "# Imputar valores NaN com a média de cada coluna - isso substituirá quaisquer NaNs que apareceram durante o dimensionamento\n",
        "!pip install sklearn\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_scaled = imputer.fit_transform(X_train_scaled)\n",
        "X_test_scaled = imputer.transform(X_test_scaled)\n",
        "\n",
        "# Verifique novamente os valores NaN nos dados dimensionados para confirmar a imputação\n",
        "print(np.isnan(X_train_scaled).any())\n",
        "print(np.isnan(X_test_scaled).any())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq_bk9uvVcEG",
        "outputId": "de611b97-cc22-40a1-8577-9a2ec6d8c5d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "False\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Regularização Ridge\n",
        "alphas = [0, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "ridge_scores = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train_scaled, y_train)\n",
        "    y_pred = ridge.predict(X_test_scaled)\n",
        "    score = r2_score(y_test, y_pred)\n",
        "    ridge_scores.append(score)\n",
        "    print(f\"Ridge (alpha={alpha}): R² = {score:.4f}\")\n",
        "\n",
        "best_ridge_alpha = alphas[np.argmax(ridge_scores)]\n",
        "print(f\"\\nMelhor modelo Ridge: alpha = {best_ridge_alpha}, R² = {max(ridge_scores):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rTXScPZUmPz",
        "outputId": "84d4a783-954b-41cc-c026-d2a2a5766b10"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge (alpha=0): R² = 0.2681\n",
            "Ridge (alpha=0.001): R² = 0.2681\n",
            "Ridge (alpha=0.005): R² = 0.2681\n",
            "Ridge (alpha=0.01): R² = 0.2681\n",
            "Ridge (alpha=0.05): R² = 0.2681\n",
            "Ridge (alpha=0.1): R² = 0.2681\n",
            "\n",
            "Melhor modelo Ridge: alpha = 0, R² = 0.2681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Regularização Lasso\n",
        "lasso_scores = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    lasso = Lasso(alpha=alpha)\n",
        "    lasso.fit(X_train_scaled, y_train)\n",
        "    y_pred = lasso.predict(X_test_scaled)\n",
        "    score = r2_score(y_test, y_pred)\n",
        "    lasso_scores.append(score)\n",
        "    print(f\"Lasso (alpha={alpha}): R² = {score:.4f}\")\n",
        "\n",
        "best_lasso_alpha = alphas[np.argmax(lasso_scores)]\n",
        "print(f\"\\nMelhor modelo Lasso: alpha = {best_lasso_alpha}, R² = {max(lasso_scores):.4f}\")\n",
        "\n",
        "# Comparação Ridge vs Lasso\n",
        "if max(ridge_scores) > max(lasso_scores):\n",
        "    print(\"\\nRidge obteve melhor resultado.\")\n",
        "else:\n",
        "    print(\"\\nLasso obteve melhor resultado.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCsNjH5gWHdF",
        "outputId": "4820f67a-6499-43ff-9214-a20ca62a53c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+11, tolerance: 7.723e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso (alpha=0): R² = 0.2681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.833e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso (alpha=0.001): R² = 0.2681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.746e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso (alpha=0.005): R² = 0.2681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.641e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso (alpha=0.01): R² = 0.2681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.936e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lasso (alpha=0.05): R² = 0.2681\n",
            "Lasso (alpha=0.1): R² = 0.2681\n",
            "\n",
            "Melhor modelo Lasso: alpha = 0, R² = 0.2681\n",
            "\n",
            "Ridge obteve melhor resultado.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.314e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Modelo Stepwise\n",
        "lr = LinearRegression()\n",
        "sfs = SequentialFeatureSelector(lr, n_features_to_select='auto', direction='forward', cv=5)\n",
        "\n",
        "# Imputar valores ausentes em X_train antes de ajustar o SFS\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "sfs.fit(X_train_imputed, y_train)\n",
        "\n",
        "X_train_sfs = sfs.transform(X_train_imputed)\n",
        "\n",
        "# Imputar valores ausentes em X_test antes de transformar\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "X_test_sfs = sfs.transform(X_test_imputed)\n",
        "\n",
        "lr.fit(X_train_sfs, y_train)\n",
        "y_pred_sfs = lr.predict(X_test_sfs)\n",
        "stepwise_score = r2_score(y_test, y_pred_sfs)\n",
        "\n",
        "print(f\"\\nModelo Stepwise: R² = {stepwise_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYxa1wNIXfd8",
        "outputId": "b3818c15-f3e3-4bfa-80e0-265b30f85227"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Modelo Stepwise: R² = 0.2691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparação final\n",
        "best_score = max(max(ridge_scores), max(lasso_scores), stepwise_score)\n",
        "if best_score == max(ridge_scores):\n",
        "    print(f\"\\nMelhor resultado geral: Ridge (alpha={best_ridge_alpha}), R² = {best_score:.4f}\")\n",
        "elif best_score == max(lasso_scores):\n",
        "    print(f\"\\nMelhor resultado geral: Lasso (alpha={best_lasso_alpha}), R² = {best_score:.4f}\")\n",
        "else:\n",
        "    print(f\"\\nMelhor resultado geral: Stepwise, R² = {best_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwo8gu-oYA4q",
        "outputId": "aa49b076-7c6a-4ffd-be6c-e038b29f8956"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Melhor resultado geral: Stepwise, R² = 0.2691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar e preparar os dados\n",
        "df = pd.read_csv(\"previsao_de_renda.csv\")\n",
        "X = pd.get_dummies(df.drop('renda', axis=1), drop_first=True)\n",
        "y = df['renda']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "EwZpo5JCY6Zx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funções auxiliares\n",
        "def print_model_results(model_name, r2, coef=None):\n",
        "    print(f\"\\n{model_name}: R² = {r2:.4f}\")\n",
        "    if coef is not None:\n",
        "        print(\"Top 5 coeficientes mais importantes:\")\n",
        "        coef_sorted = sorted(zip(X.columns, coef), key=lambda x: abs(x[1]), reverse=True)\n",
        "        for name, value in coef_sorted[:5]:\n",
        "            print(f\"{name}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "b3M2Bt5NY25x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge e Lasso\n",
        "alphas = [0, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "best_ridge, best_lasso = None, None\n",
        "best_ridge_r2, best_lasso_r2 = -np.inf, -np.inf\n",
        "\n",
        "# Imputar valores ausentes em X_train_scaled antes de ajustar os modelos\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_scaled = imputer.fit_transform(X_train_scaled)\n",
        "\n",
        "for alpha in alphas:\n",
        "    for model_class in [Ridge, Lasso]:\n",
        "        model = model_class(alpha=alpha)\n",
        "\n",
        "        #Ajustar o modelo aos dados imputados\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Imputar valores ausentes em X_test_scaled antes de prever\n",
        "        X_test_scaled = imputer.transform(X_test_scaled)\n",
        "\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        if model_class == Ridge and r2 > best_ridge_r2:\n",
        "            best_ridge, best_ridge_r2 = model, r2\n",
        "        elif model_class == Lasso and r2 > best_lasso_r2:\n",
        "            best_lasso, best_lasso_r2 = model, r2\n",
        "\n",
        "print_model_results(\"Best Ridge\", best_ridge_r2, best_ridge.coef_)\n",
        "print_model_results(\"Best Lasso\", best_lasso_r2, best_lasso.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuNDkfJcZSjc",
        "outputId": "87cfbc46-bf81-4826-8e87-5bec78657719"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+11, tolerance: 7.723e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.833e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.746e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.641e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.936e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Ridge: R² = 0.2681\n",
            "Top 5 coeficientes mais importantes:\n",
            "tempo_emprego: 3728.8335\n",
            "sexo_M: 2733.2979\n",
            "qt_pessoas_residencia: 1025.8152\n",
            "Unnamed: 0: -895.9723\n",
            "qtd_filhos: -769.0141\n",
            "\n",
            "Best Lasso: R² = 0.2681\n",
            "Top 5 coeficientes mais importantes:\n",
            "tempo_emprego: 3728.8443\n",
            "sexo_M: 2733.1818\n",
            "qt_pessoas_residencia: 1018.1999\n",
            "Unnamed: 0: -768.4793\n",
            "qtd_filhos: -762.7691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.314e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stepwise\n",
        "lr = LinearRegression()\n",
        "# Imputar valores ausentes em X_train antes de ajustar o SFS\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "sfs = SequentialFeatureSelector(lr, n_features_to_select='auto', direction='forward', cv=5)\n",
        "sfs.fit(X_train_imputed, y_train)\n",
        "\n",
        "X_train_sfs = sfs.transform(X_train_imputed)\n",
        "# Imputar valores ausentes em X_test antes de transformar\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "X_test_sfs = sfs.transform(X_test_imputed)\n",
        "lr.fit(X_train_sfs, y_train)\n",
        "y_pred_sfs = lr.predict(X_test_sfs)\n",
        "stepwise_r2 = r2_score(y_test, y_pred_sfs)\n",
        "print_model_results(\"Stepwise\", stepwise_r2, lr.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1pVSm3eZc0N",
        "outputId": "8d3a0e47-d0c6-43e8-e72c-d1ef0533807f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stepwise: R² = 0.2691\n",
            "Top 5 coeficientes mais importantes:\n",
            "data_ref_2015-02-01: 5839.0880\n",
            "data_ref_2015-05-01: -1452.7469\n",
            "data_ref_2015-03-01: -1075.1550\n",
            "data_ref_2015-06-01: 999.9734\n",
            "data_ref_2015-04-01: 862.6125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Melhorando o modelo\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "# Imputar valores ausentes em X_train antes de ajustar\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_train_poly = poly.fit_transform(X_train_imputed)\n",
        "\n",
        "# Imputar valores ausentes em X_test antes de transformar\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "X_test_poly = poly.transform(X_test_imputed)\n",
        "\n",
        "ridge_poly = Ridge(alpha=0.1)\n",
        "ridge_poly.fit(X_train_poly, y_train)\n",
        "y_pred_poly = ridge_poly.predict(X_test_poly)\n",
        "poly_r2 = r2_score(y_test, y_pred_poly)\n",
        "print_model_results(\"Polynomial Ridge\", poly_r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_EYkceqZ0gH",
        "outputId": "8edb6240-17bd-43b5-d9d2-1b945e68cfe0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Polynomial Ridge: R² = 0.3532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.27705e-22): result may not be accurate.\n",
            "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Árvore de Regressão\n",
        "tree = DecisionTreeRegressor(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "tree_r2 = r2_score(y_test, y_pred_tree)\n",
        "print_model_results(\"Decision Tree\", tree_r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHGtCB_KZ4pW",
        "outputId": "006d9c71-d12e-42fb-e78e-2c0e4432a7ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree: R² = 0.1438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest (como bônus)\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Imputar valores ausentes em X_train antes de ajustar o modelo\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "# Ajustar o modelo aos dados imputados\n",
        "rf.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Imputar valores ausentes em X_test antes de prever\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "y_pred_rf = rf.predict(X_test_imputed)\n",
        "rf_r2 = r2_score(y_test, y_pred_rf)\n",
        "print_model_results(\"Random Forest\", rf_r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDmoGsn6aC5o",
        "outputId": "6311a06d-34cd-411a-fbb3-4dc3643a4049"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest: R² = 0.4159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparação final\n",
        "models = {\n",
        "    \"Ridge\": best_ridge_r2,\n",
        "    \"Lasso\": best_lasso_r2,\n",
        "    \"Stepwise\": stepwise_r2,\n",
        "    \"Polynomial Ridge\": poly_r2,\n",
        "    \"Decision Tree\": tree_r2,\n",
        "    \"Random Forest\": rf_r2\n",
        "}\n",
        "\n",
        "best_model = max(models, key=models.get)\n",
        "print(f\"\\nMelhor modelo: {best_model} com R² = {models[best_model]:.4f}\")\n",
        "\n",
        "print(\"\\nMinha opinião sobre o melhor modelo:\")\n",
        "print(\"Considerando o equilíbrio entre desempenho (R²) e interpretabilidade, eu escolheria...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u9xqYnEaIGL",
        "outputId": "f83b9fed-a12d-47f5-a7e5-0e501deab9b9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Melhor modelo: Random Forest com R² = 0.4159\n",
            "\n",
            "Minha opinião sobre o melhor modelo:\n",
            "Considerando o equilíbrio entre desempenho (R²) e interpretabilidade, eu escolheria...\n"
          ]
        }
      ]
    }
  ]
}